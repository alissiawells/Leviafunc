{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP of legal texts\n",
    "Analysis of agreements between governments \n",
    "* Key words & key phrases extraction with TF-IDF and N-gramms\n",
    "* NER for DATES with ([Natasha](https://natasha.github.io/demo/) (rule-based lib for Russian language). Sequence model, implemented in [AnaGo](https://anago.herokuapp.com/) and NER by [DeepMIPT](https://demo.ipavlov.ai/) have lower accuracy for this type of text.\n",
    "* Dictionary method and morphological analysis for finding ORGANIZATIONS and COUNTRIES (accuracy is more important than the opportunity to expand the lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import date\n",
    "import pymorphy2\n",
    "import gensim\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "m = pymorphy2.MorphAnalyzer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from natasha import (\n",
    "    NamesExtractor,\n",
    "    DatesExtractor,\n",
    "    MoneyExtractor,\n",
    "    LocationExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemm(word):\n",
    "    word = re.sub(\"(</?.*?>)|(<>)|(\\\\d|\\\\W)+\", '', word).lower()\n",
    "    return m.parse(word)[0].normal_form.strip()\n",
    "def preprocess(file_readed_by_lines):\n",
    "    return [[lemm(word) for word in word_tokenize(text) if ((lemm(word) not in stopWords) and len(word)>3)] for text in file_readed_by_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus is uploaded from https://xn--80abucjiibhv9a.xn--p1ai/%D0%BC%D0%B8%D0%BD%D0%B8%D1%81%D1%82%D0%B5%D1%80%D1%81%D1%82%D0%B2%D0%BE/68/%D1%84%D0%B0%D0%B9%D0%BB/916/%D0%9C%D0%A1_%D0%9D%D0%A2%D0%A1.pdf\n",
    "# doesn't contain the test doc\n",
    "with open (\"corpus.txt\", \"r\") as f:\n",
    "    corpus_lines = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"corpus.txt\", \"r\") as f:\n",
    "    corpus = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"testdoc.txt\", \"r\") as f:\n",
    "    doc_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"testdoc.txt\", \"r\") as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(line.strip() for line in open('RUstopwords.txt', 'r'))\n",
    "# can be expanded\n",
    "print(len(stopWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "countries = set(line.strip().lower() for line in open('countries.txt', 'r'))\n",
    "# can be expanded\n",
    "print(len(countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    }
   ],
   "source": [
    "organizations = set(line.strip().lower() for line in open('organizations.txt', 'r'))\n",
    "# can be expanded\n",
    "print(len(organizations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = preprocess(corpus_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf_doc = preprocess(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "for line in df_idf:\n",
    "    corpus += \" \".join(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\n",
    "for line in df_idf_doc:\n",
    "    doc += \" \".join(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing TF-IDF and extracting key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sorts the values in the vector while preserving the column index\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key = lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=20):\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def tfidf_keywords(corpus, doc):\n",
    "    \n",
    "    cv = CountVectorizer(max_df=0.85)\n",
    "    word_count_vector = cv.fit_transform(word_tokenize(corpus))\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count_vector)\n",
    "    feature_names = cv.get_feature_names()\n",
    "    \n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([doc])) # enumerates a vector of tf-idf scores\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    return extract_topn_from_vector(feature_names,sorted_items,20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "kw_euristics = [[word for word in line if (word[-3:]==\"ция\")] for line in df_idf_doc]\n",
    "for line in kw_euristics:\n",
    "    for word in line:\n",
    "        keywords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords += tfidf_keywords(corpus, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key phrases with tf-idf using N-gramms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def words_to_bigramms(text, str_bigrams = \"\"):\n",
    "    for line in text:\n",
    "        bigrams = ngrams(line,2)\n",
    "        for k1, k2 in Counter(bigrams):\n",
    "            str_bigrams += k1+ \"_\" + k2+ \"_\" + \" \"\n",
    "    return str_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_trigramms(text, str_trigrams = \"\"):\n",
    "    for line in text:\n",
    "        trigrams = ngrams(line,3)\n",
    "        for k1, k2,k3 in Counter(trigrams):\n",
    "            str_trigrams += k1+ \"_\" + k2+ \"_\" +k3 + \" \"\n",
    "    return str_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = list(tfidf_keywords(words_to_bigramms(df_idf), words_to_bigramms(df_idf_doc)))\n",
    "keyphrases += list(tfidf_keywords(words_to_trigramms(df_idf), words_to_bigramms(df_idf_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Название документа: СОГЛАШЕНИЕ между Правительством Российской Федерации и Правительством Федеративной Республики Бразилии о научно-техническом сотрудничестве\n",
      "\n",
      "Страны: Бразилия\n",
      "\n",
      "Вид документа: Двустороннее соглашение\n",
      "\n",
      "Направление: Научно-техническое сотрудничество\n",
      "Область: Научно-техническое сотрудничество\n",
      "\n",
      "Дата заключения: 1997-11-21\n",
      "\n",
      "Дата вступления в силу: 1999-09-30\n",
      "\n",
      "*Документ также содержит даты в неполном формате (2002, 12, None)\n",
      "\n",
      "Ключевые слова:\n",
      "страна\n",
      "реализация\n",
      "проект\n",
      "научнотехнический\n",
      "сила\n",
      "информация\n",
      "организация\n",
      "бразилия\n",
      "ассоциация\n",
      "федеративный\n",
      "условие\n",
      "координация\n",
      "бразилиа\n",
      "федерация\n",
      "сторона\n",
      "соглашение\n",
      "комиссия\n",
      "сотрудничество\n",
      "правительство\n",
      "отношение\n",
      "настоящий\n",
      "действие\n",
      "каждый\n",
      "настоящее\n",
      "республика\n",
      "рекомендация\n",
      "\n",
      "Наиболее часто встречающиеся выражения (ngramms):\n",
      "декабрь__\n",
      "документ_упомянуть_\n",
      "сотрудничать_организация_\n",
      "каждый_конкретный_\n",
      "правительство_федеративный_\n",
      "благоприятный_условие_\n",
      "научнотехнический_сотрудничество_\n",
      "настоящий_соглашение_\n",
      "конкретный_случай_\n",
      "_год_\n",
      "бразилиа_ноябрь_\n",
      "бразилия_апрель_\n",
      "создание_благоприятный_\n",
      "федеративный_республика_\n",
      "упомянуть_статья_\n",
      "настоящее_соглашение_\n",
      "результат_сотрудничество_\n",
      "федерация_федеративный_\n",
      "республика_бразилия_\n",
      "согласовать_сторона_\n",
      "российский_федерация_\n",
      "научнотехнический_связь_\n",
      "оба_страна_\n",
      "закон_правило_\n"
     ]
    }
   ],
   "source": [
    "# euristics and morphological analysis\n",
    "title = str(doc_lines[0]).strip()\n",
    "topic = title.split(' ')[-2:]   \n",
    "noun = m.parse(topic[1])[0]\n",
    "adj = m.parse(topic[0])[0].inflect({noun.tag.gender, 'sing', 'nomn'})\n",
    "print(\"\\nНазвание документа: %s%s\" % (title[0].upper(), title[1:]))\n",
    "\n",
    "# dictionary method - искать также на три граммах и двуграммах (список строк)\n",
    "df_idf_doc = preprocess(doc_lines)\n",
    "orgs = []\n",
    "orgs_ = [[word for word in line if (word in organizations)] for line in df_idf_doc]\n",
    "for line in orgs_:\n",
    "    for word in line:\n",
    "        orgs.append(word)\n",
    "for word in set(orgs):\n",
    "    print(\"\\nОрганизации:\", word[0].upper()+word[1:])\n",
    "    \n",
    "# dictionary method - искать также на три граммах и двуграммах\n",
    "coun_euristics = []\n",
    "coun = [[word for word in line if (word in countries)] for line in df_idf_doc]\n",
    "for line in coun:\n",
    "    for word in line:\n",
    "        coun_euristics.append(word)\n",
    "for word in set(coun_euristics):\n",
    "    print(\"\\nСтраны:\", word[0].upper()+word[1:])\n",
    "\n",
    "# euristics and morphological analysis\n",
    "act_type = [\"в рамках\", \"содружества\", \"государств-участников\", \"межгосударственном\"]\n",
    "title_lemm = ' '.join([lemm(word) for word in word_tokenize(title)])\n",
    "\n",
    "for word in act_type:\n",
    "    if lemm(word) in title_lemm:\n",
    "        type = \"Многостороннее соглашение\"\n",
    "    else:\n",
    "        type = \"Двустороннее соглашение\"\n",
    "print(\"\\nВид документа: %s\" % type) \n",
    "          \n",
    "print(\"\\nНаправление:\", adj[0][0].upper()+adj[0][1:].lower(), lemm(noun[0]))\n",
    "print(\"Область:\", adj[0][0].upper()+adj[0][1:].lower(), lemm(noun[0]))\n",
    "\n",
    "# NER\n",
    "dates = []\n",
    "attention = set()\n",
    "extractor = DatesExtractor()\n",
    "for line in doc_lines:\n",
    "    matches = extractor(line)\n",
    "    for index, match in enumerate(matches):\n",
    "        try:\n",
    "            dates.append(date(match.fact.year, match.fact.month, match.fact.day))\n",
    "        except TypeError as e:\n",
    "            attention = match.fact.year, match.fact.month, match.fact.day\n",
    "# usually acts with earlier dates are denied or are the ones which the current document is based on\n",
    "data2 = dates.pop(dates.index(max(dates)))\n",
    "data1 = max(dates);\n",
    "print(\"\\nДата заключения:\", data1)\n",
    "print(\"\\nДата вступления в силу:\", data2)\n",
    "if len(attention) > 0:\n",
    "    print(\"\\n*Документ также содержит даты в неполном формате\", attention)\n",
    "    \n",
    "print(\"\\nКлючевые слова:\")\n",
    "for word in set(keywords):\n",
    "    print(word)\n",
    "\n",
    "print(\"\\nНаиболее часто встречающиеся выражения (ngramms):\")\n",
    "for phrase in set(keyphrases):\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
